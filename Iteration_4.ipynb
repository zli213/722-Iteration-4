{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60e431d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.1-bin-hadoop2.7/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/17 11:57:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Must be included at the beginning of each new notebook. Remember to change the app name.\n",
    "import findspark\n",
    "findspark.init('/home/ubuntu/spark-3.2.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StringType, StructType, StructField, Row, IntegerType, ArrayType\n",
    "spark = SparkSession.builder.appName('Iteration_4').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf2cc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12218, 12217)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's read in the data. Note that it's in the format of JSON.\n",
    "df01 = spark.read.csv('Datasets/job_postings.csv', header=True, inferSchema=True)\n",
    "df02 = spark.read.csv('Datasets/job_skills.csv', header=True, inferSchema=True)\n",
    "df01_count = df01.count()\n",
    "df02_count = df02.count()\n",
    "(df01_count,df02_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92efd0b",
   "metadata": {},
   "source": [
    "## Use pandas to try again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "649f5c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12217, 12217)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_postings_df = pd.read_csv('Datasets/job_postings.csv', dtype=str)\n",
    "job_skills_df = pd.read_csv('Datasets/job_skills.csv', dtype=str)\n",
    "\n",
    "job_postings_count = len(job_postings_df)\n",
    "job_skills_count = len(job_skills_df)\n",
    "(job_postings_count,job_skills_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b44619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df01行数: 12218, df02行数: 12217\n",
      "Display extral rows:\n",
      "+--------------------+--------------------+--------------------+-----------+-------+---------------+--------------------+----------+------------+----------+-----------+--------------+---------------+---------+--------+\n",
      "|            job_link| last_processed_time|         last_status|got_summary|got_ner|is_being_worked|           job_title|   company|job_location|first_seen|search_city|search_country|search_position|job_level|job_type|\n",
      "+--------------------+--------------------+--------------------+-----------+-------+---------------+--------------------+----------+------------+----------+-----------+--------------+---------------+---------+--------+\n",
      "|                 ...|RED Engineering D...|London, England, ...| 2024-01-15| Slough| United Kingdom|Electrical Engine...|Mid senior|      Onsite|      null|       null|          null|           null|     null|    null|\n",
      "+--------------------+--------------------+--------------------+-----------+-------+---------------+--------------------+----------+------------+----------+-----------+--------------+---------------+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "common_columns = list(set(df01.columns).intersection(set(df02.columns)))\n",
    "\n",
    "# Find common columns\n",
    "df01_common = df01.select(common_columns)\n",
    "df02_common = df02.select(common_columns)\n",
    "\n",
    "# Gain rows\n",
    "df01_count = df01_common.count()\n",
    "df02_count = df02_common.count()\n",
    "print(f'df01行数: {df01_count}, df02行数: {df02_count}')\n",
    "\n",
    "# Find extral rows\n",
    "diff_df = df01.join(df02, on='job_link', how='left_anti')\n",
    "\n",
    "# Display extral rows\n",
    "print(\"Display extral rows:\")\n",
    "diff_df.show()\n",
    "\n",
    "# Delete extral rows\n",
    "df01_filtered = df01.subtract(diff_df)\n",
    "\n",
    "df01_filtered.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e7d970",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe64410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise DataFrames df1 \n",
    "\n",
    "df01_filtered.show()\n",
    "df01_filtered.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5227bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise DataFrames df2\n",
    "df02.show()\n",
    "df02.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8525bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "pd_df01 = df01_filtered.toPandas()\n",
    "pd_df02 = df02.toPandas()\n",
    "# Display summary statistics using Pandas\n",
    "from IPython.display import display\n",
    "display(pd_df01.describe())\n",
    "display(pd_df02.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775cb0f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ebf0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For type, we can use print schema. \n",
    "df01_filtered.printSchema()\n",
    "df02.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30aff549",
   "metadata": {},
   "source": [
    "## Data Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdde4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df01_clean = df01.na.drop(subset=\"job_link\")\n",
    "pd_df01_clean = df01_clean.toPandas()\n",
    "display(pd_df01_clean.describe(include='all'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fd94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf, split, explode, count\n",
    "import re\n",
    "# Use .distinct() and .exceptAll() to check if there are any unmatched 'job_link' entries between the two DataFrames\n",
    "different_links = df01_filtered.select(\"job_link\").distinct().exceptAll(\n",
    "    df02.select(\"job_link\").distinct()\n",
    ")\n",
    "\n",
    "# Decide whether to merge based on the presence of unmatched 'job_link' entries\n",
    "if different_links.count() == 0:\n",
    "    # If there are no unmatched 'job_link' entries, proceed with merging\n",
    "    merged_data = df01_filtered.join(df02, \"job_link\", \"inner\")\n",
    "    print(\"DataFrames merged successfully.\")\n",
    "else:\n",
    "    # If there are unmatched 'job_link' entries, do not merge\n",
    "    print(\"DataFrames' job_link columns are not the same, cannot merge.\")\n",
    "\n",
    "if different_links.count() > 0:\n",
    "    print(\"Unmatched job_link entries:\")\n",
    "    different_links.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123ff87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.show()\n",
    "merged_data.printSchema()\n",
    "\n",
    "# Display summary statistics using Pandas\n",
    "pd_merged_data = merged_data.toPandas()\n",
    "display(pd_merged_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean = merged_data.na.drop()\n",
    "# Display summary statistics using Pandas\n",
    "pd_df_clean = df_clean.toPandas()\n",
    "display(pd_df_clean.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ca10cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除指定的列\n",
    "columns_to_drop = ['last_processed_time', 'last_status', 'job_link', 'got_summary', 'got_ner',\n",
    "                   'is_being_worked', 'company', 'first_seen', 'search_city', 'search_position', 'job_type']\n",
    "filtered_df = merged_data.drop(*columns_to_drop)\n",
    "\n",
    "# 显示过滤后的DataFrame\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fda31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数\n",
    "def split_location(location):\n",
    "    if location is None:\n",
    "        return Row(city=None, state=None, country=None)\n",
    "\n",
    "    parts = [part.strip() for part in str(location).split(',')]\n",
    "    city = state = country = None\n",
    "\n",
    "    known_countries = [\"United States\", \"United Kingdom\", \"Canada\", \"Australia\", \"India\",\n",
    "                       \"Germany\", \"France\", \"Italy\", \"Spain\", \"Mexico\"]\n",
    "    us_states = {\n",
    "        'AL': 'Alabama', 'AK': 'Alaska', 'AZ': 'Arizona', 'AR': 'Arkansas', 'CA': 'California',\n",
    "        'CO': 'Colorado', 'CT': 'Connecticut', 'DE': 'Delaware', 'FL': 'Florida', 'GA': 'Georgia',\n",
    "        'HI': 'Hawaii', 'ID': 'Idaho', 'IL': 'Illinois', 'IN': 'Indiana', 'IA': 'Iowa',\n",
    "        'KS': 'Kansas', 'KY': 'Kentucky', 'LA': 'Louisiana', 'ME': 'Maine', 'MD': 'Maryland',\n",
    "        'MA': 'Massachusetts', 'MI': 'Michigan', 'MN': 'Minnesota', 'MS': 'Mississippi', 'MO': 'Missouri',\n",
    "        'MT': 'Montana', 'NE': 'Nebraska', 'NV': 'Nevada', 'NH': 'New Hampshire', 'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico', 'NY': 'New York', 'NC': 'North Carolina', 'ND': 'North Dakota', 'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma', 'OR': 'Oregon', 'PA': 'Pennsylvania', 'RI': 'Rhode Island', 'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota', 'TN': 'Tennessee', 'TX': 'Texas', 'UT': 'Utah', 'VT': 'Vermont',\n",
    "        'VA': 'Virginia', 'WA': 'Washington', 'WV': 'West Virginia', 'WI': 'Wisconsin', 'WY': 'Wyoming', 'DC': 'Washington'\n",
    "    }\n",
    "    # Create a reverse map from full state name to abbreviation\n",
    "    state_abbrev = {v: k for k, v in us_states.items()}\n",
    "\n",
    "    # If there is only one part\n",
    "    if len(parts) == 1:\n",
    "        if parts[0] in known_countries:\n",
    "            country = parts[0]\n",
    "        else:\n",
    "            state = parts[0]  # Translate to state if not in known countries\n",
    "    elif len(parts) == 2:\n",
    "        if parts[1] in known_countries:\n",
    "            city = parts[0]\n",
    "            country = parts[1]\n",
    "            if city in state_abbrev and country == \"United States\":\n",
    "                state = state_abbrev[city]\n",
    "                city = None\n",
    "            else:\n",
    "                state = city\n",
    "                city = None\n",
    "        else:\n",
    "            city, state = parts\n",
    "            if state in us_states.keys():  # If the state name is the abbreviation of the US state\n",
    "                country = \"United States\"  # Country name to United States\n",
    "    elif len(parts) == 3:\n",
    "        city, state, country = parts\n",
    "        if state in state_abbrev:  # If state is full name\n",
    "            state = state_abbrev[state]  # To abbreviation\n",
    "\n",
    "    return Row(city=city, state=state, country=country)\n",
    "\n",
    "# 注册UDF\n",
    "split_location_udf = udf(lambda location: split_location(location), StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True)\n",
    "]))\n",
    "\n",
    "# 应用UDF\n",
    "df_with_location = filtered_df.withColumn(\"location_split\", split_location_udf(col(\"job_location\")))\n",
    "df_with_location = df_with_location.select(\"*\", \"location_split.city\", \"location_split.state\", \"location_split.country\").drop(\"location_split\")\n",
    "\n",
    "# 显示过滤后的DataFrame\n",
    "df_with_location.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683840e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义技能规范化函数\n",
    "def normalize_skill_name(skill):\n",
    "    skill_normalized = skill.strip().lower()\n",
    "    if skill_normalized in (\"go\", \"golang\"):\n",
    "        return \"Go/Golang\"\n",
    "    if skill_normalized == \"ruby on rails\":\n",
    "        return \"Ruby\"\n",
    "    if skill_normalized in (\"ai\", \"llms\", \"artificial intelligence\", \"generative ai\", \"natural language processing\",\n",
    "                            \"nlp\", 'machine learning', 'deep learning', 'ml', 'reinforcement learning'):\n",
    "        return 'ML/AI'\n",
    "    if 'problem' in skill_normalized and 'solving' in skill_normalized:\n",
    "        return 'ProblemSolving'\n",
    "    if 'microsoft' in skill_normalized and ('office' in skill_normalized or 'excel' in skill_normalized or 'Excel' in skill_normalized):\n",
    "        return 'MicrosoftOffice'\n",
    "    if re.search(r'\\b(cloud|azure|aws|gcp)\\b', skill_normalized, re.IGNORECASE):\n",
    "        return 'Cloud Computing'\n",
    "    if 'communication' in skill_normalized:\n",
    "        return 'Communication'\n",
    "    if re.search(r'\\b(software|programming|developer)\\b', skill_normalized, re.IGNORECASE):\n",
    "        return 'SoftwareDevelopment'\n",
    "    if re.search(r'\\b(data science|data scientist)\\b', skill_normalized, re.IGNORECASE):\n",
    "        return 'DataScience'\n",
    "    if re.search(r'\\b(data analysis|data analytics|data_analysis)\\b', skill_normalized, re.IGNORECASE):\n",
    "        return 'DataAnalysis'\n",
    "    if 'java' in skill_normalized and 'script' not in skill_normalized:\n",
    "        return 'Java'\n",
    "    if 'javascript' in skill_normalized or 'typescript' in skill_normalized:\n",
    "        return 'JavaScript/TypeScript'\n",
    "    if ' ' in skill_normalized:\n",
    "        return ''.join([word.capitalize() for word in skill_normalized.split()])\n",
    "    else:\n",
    "        return skill_normalized.capitalize()\n",
    "\n",
    "# 注册UDF\n",
    "normalize_skill_name_udf = udf(normalize_skill_name, StringType())\n",
    "\n",
    "# 拆分技能列\n",
    "df_with_skills = df_with_location.withColumn(\"skills\", explode(split(col(\"job_skills\"), \",\")))\n",
    "\n",
    "# 规范化技能名称\n",
    "df_with_skills_normalized = df_with_skills.withColumn(\"normalized_skill\", normalize_skill_name_udf(col(\"skills\")))\n",
    "\n",
    "# 计算技能频率并排序\n",
    "skill_counts = df_with_skills_normalized.groupBy(\"normalized_skill\").agg(count(\"*\").alias(\"count\")).orderBy(col(\"count\").desc())\n",
    "\n",
    "# 显示技能频率\n",
    "skill_counts.show()\n",
    "\n",
    "# 处理分类列\n",
    "categories = {\n",
    "    'Cloud': ['cloud', 'aws', 'azure', 'google cloud', 'cloud engineer', 'data center', 'cloud architect', 'datacenter'],\n",
    "    'Testing': ['test', 'tester', 'testing', 'quality assurance', 'qa', 'test engineer'],\n",
    "    'Development': ['developer', 'development', 'software engineer', 'programmer', 'solution architect', 'database developer',\n",
    "                    'software', 'software development', \"C++\", \"Java\", \"C#\", \"JavaScript/TypeScript\", \"Go/Golang\", \"Visual Basic\",\n",
    "                    \"Assembly language\", \"PHP\", \"Delphi/Object Pascal\", \"Swift\", \"Rust\", \"Ruby\",\n",
    "                    \"Kotlin\", \"COBOL\"],\n",
    "    'Data_science': ['data science', 'machine learning', 'ml', 'deep learning', 'ai', 'artificial intelligence', 'data scientist', 'analytics architect', 'data analytics', 'npl'],\n",
    "    'Data_analysis': ['data analyst', 'data analysis', 'business intelligence', 'data reporting', 'financial data', 'data warehouse', 'data mining', 'data architect'],\n",
    "    'Devops': ['devops engineer', 'site reliability', 'sre', 'automation engineer', 'infrastructure as code', 'ci/cd', 'release engineer']\n",
    "}\n",
    "\n",
    "# 为每个类别创建新列并初始化为0\n",
    "for category in categories:\n",
    "    df_with_skills_normalized = df_with_skills_normalized.withColumn(category, lit(0))\n",
    "\n",
    "# 定义一个函数来检查技能是否属于某个类别\n",
    "def is_in_category(skill, keywords):\n",
    "    return any(keyword in skill for keyword in keywords)\n",
    "\n",
    "# 注册UDF\n",
    "is_in_category_udf = udf(lambda skill, keywords: is_in_category(skill, keywords), IntegerType())\n",
    "\n",
    "# 检查 job_title 和其他相关列\n",
    "for category, keywords in categories.items():\n",
    "    df_with_skills_normalized = df_with_skills_normalized.withColumn(category, udf(\n",
    "        lambda job_title: int(any(keyword in job_title for keyword in keywords)), IntegerType())(col(\"job_title\")))\n",
    "\n",
    "# 更新分类列的值\n",
    "for category, keywords in categories.items():\n",
    "    formatted_keywords = [''.join(word.capitalize() for word in keyword.split()) for keyword in keywords]\n",
    "    column_patterns = [f\"{i + 1}. {formatted_keyword}\".lower() for i, formatted_keyword in enumerate(formatted_keywords)]\n",
    "    matching_columns = [col for col in df_with_skills_normalized.columns if any(col.lower().endswith(pattern) for pattern in column_patterns)]\n",
    "    \n",
    "    for col_name in matching_columns:\n",
    "        df_with_skills_normalized = df_with_skills_normalized.withColumn(category, udf(\n",
    "            lambda val, current_val: max(val, current_val), IntegerType())(col(col_name), col(category)))\n",
    "\n",
    "# 删除不需要的列\n",
    "df_final = df_with_skills_normalized.drop('job_title', 'search_country', 'job_location', 'city', 'job_skills')\n",
    "\n",
    "# 显示最终DataFrame\n",
    "df_final.show()\n",
    "\n",
    "pd_df_final = df_final.toPandas()\n",
    "display(pd_df_final.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a28a18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
